{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ad1f79-f67c-4375-ac5c-840b3c6b498c",
   "metadata": {},
   "source": [
    "# Personality clustering\n",
    "\n",
    "Topic: Using Logic Tensor Networks in order to identify and create clusters that represent different types of personalities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd63e27-b305-4e49-a7f6-3684c3d1c70d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Why Logic Tensor Networs?\n",
    "\n",
    "Logic Tensor Networks is a framework that supports querying, learning and reasoning. Just by defining some logical constraints, we can let the neural network learn without the need to pass in the labels.\n",
    "There are many interesting ways of defining those types of constraints, which we will analyse in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1261b-5475-4364-b6bf-47e76c74819b",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset we will be using is the following: https://www.kaggle.com/datasets/anshulmehtakaggl/60k-responses-of-16-personalities-test-mbt. Unfortunately, it is a synthetic generated dataset, so the results might differ from real world.\n",
    "This database contains answers for 60 personality questions, along with the results. For simplicity, we will train our LTN on 1000 entries of 4 different personalities: ESFP (Entertainer), ENTJ (commander), INFP (Mediator), ISTJ (Logistician)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5fc67-c1b9-44a6-a1a5-dbd54d9020e3",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "Each answer in the dataset has 5 possible values:\n",
    "* -3: Fully disagree\n",
    "* -2: Partially disagree\n",
    "* -1: Slightly disagree\n",
    "*  0: Neutral\n",
    "*  1: Slightly agree\n",
    "*  2: Partially agree\n",
    "*  3: Fully agree\n",
    "\n",
    "We start from the premise that people with similar personalities will answer each question in a similar fashion. The more different the answers, the higher the probability that they have different personalities. We will introduce the concept of \"penalty\": the more different the answer, the higher the penalty should be. A good way of defining the penalty on a question is by taking the answer difference to the power of 2.\n",
    "\n",
    "Example:\n",
    "Question: \"You regularly make new friends\"  \n",
    "Jack's answer: 2 (partially agree)  \n",
    "Tom's answer: 0 (neutral)  \n",
    "Emily's answer: -1 (slightly disagree)  \n",
    "\n",
    "We can conclude that Tom and Emily are quite similar (penalty of 1), that Tom and Jack are not completely different (penalty of 2^2 = 4), but Emily and Jack are quite far away (penalty of 3^2 = 9).\n",
    "\n",
    "The good part about this penalty system is that it allows us to use the euclidean distance between the arrays of answers as a starting point when clustering: the smaller the distance, the higher the chance to be in the same cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99333c1-6caa-4f04-ba6e-18e3c4006757",
   "metadata": {},
   "source": [
    "## Coding\n",
    "\n",
    "This code was built similar to the examples and tutorials from the official Logic Tensor Network pages: \n",
    "* https://github.com/logictensornetworks/logictensornetworks \n",
    "* https://github.com/logictensornetworks/LTNtorch  \n",
    "Some of the examples were a bit outdated, but they let us see different approaches to clustering and managing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55296361-0095-47e5-9dfe-d2f6ffde3820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 15:43:15.509488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/anne/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import logging; logging.basicConfig(level=logging.INFO)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ltn\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d85bf3-de30-4870-bbc1-39027601ea83",
   "metadata": {},
   "source": [
    "### Data fetching\n",
    "We will parse the '16P.csv' file that can be downloaded from this link: https://www.kaggle.com/datasets/anshulmehtakaggl/60k-responses-of-16-personalities-test-mbt.  \n",
    "As said earlier, we will work initially only on 1000 entries, with 4 types of personalities.\n",
    "We will also split the data into batches, cut off the entry id and cut off the labels. We will however keep those labels in a list, so we can later test the accuracy of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fba391c-66a4-4cd9-906a-2dd8c0407571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_file_path = '16P.csv'\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='ISO-8859-1', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    header = next(csv_reader, None)\n",
    "    \n",
    "    current_line = 1\n",
    "    data = [row for row in csv_reader]\n",
    "\n",
    "\n",
    "data_to_train_on = []\n",
    "labels = []\n",
    "for row in data:\n",
    "    row_to_float = []\n",
    "    if row[-1] in ['ISTP', 'ENFJ', 'ENTP', 'ISFP']:\n",
    "        for i in range(1, len(row)-1):\n",
    "            row_to_float.append(float(row[i]))\n",
    "        labels.append(row[-1])\n",
    "        data_to_train_on.append(row_to_float)\n",
    "    \n",
    "data_to_train_on = data_to_train_on[:1000]\n",
    "labels = labels[:1000]\n",
    "\n",
    "data_to_train_on = np.array(data_to_train_on)\n",
    " \n",
    "batch_size = 300    \n",
    "current_batch = []\n",
    "batches = []\n",
    " \n",
    "for rownum in range(len(data_to_train_on)):\n",
    "    if rownum % batch_size == 0 and rownum > 0:\n",
    "        batches.append(current_batch)\n",
    "        current_batch = []\n",
    "    current_batch.append(data_to_train_on[rownum])\n",
    " \n",
    "if len(current_batch):\n",
    "    batches.append(current_batch)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d6b63-474e-4e33-bd0a-329ab05de74f",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "We build the classifier for our network. We are defining the sizes of the hidden layers, the activation function (we chose elu in this case).\n",
    "We can use the softmax function so that the output nodes have a total probability of 1. If there one can have more personalities (not the case here), we can use sigmoid instead.\n",
    "We specify the number of clusters: same as the number of personalities, 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "304e5d1f-c1e2-48f1-82f2-734ddd287eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    " \n",
    "class MLP_classifier(tf.keras.Model):\n",
    "    \"\"\" Model to call as P(x,class) \"\"\"\n",
    "    def __init__(self, n_classes, single_label, hidden_layer_sizes=(32, 32, 32)):\n",
    "        super(MLP_classifier, self).__init__()\n",
    "        self.denses = [layers.Dense(s, activation=\"elu\") for s in hidden_layer_sizes]\n",
    "        self.dense_class = layers.Dense(n_classes)\n",
    "        self.to_probs = tf.nn.softmax if single_label else tf.math.sigmoid\n",
    " \n",
    "    def call(self, inputs):\n",
    "        x, c = inputs[0], inputs[1]\n",
    "        for dense in self.denses:\n",
    "            x = dense(x)\n",
    "        logits = self.dense_class(x)\n",
    "        probs = self.to_probs(logits)\n",
    "        indices = tf.cast(c, tf.int32)\n",
    "        return tf.gather(probs, indices, batch_dims=1) \n",
    " \n",
    "C = ltn.Predicate(MLP_classifier(4, single_label=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b3153-3397-4902-b61f-ed511452037e",
   "metadata": {},
   "source": [
    "Declaring a list of clusters. Notice the use of <code>ltn.Variable</code> to ground this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f4177d-2578-426e-81f7-e0dd32e6d57f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clst_ids = range(4)\n",
    "\n",
    "cluster = ltn.Variable(\"cluster\", clst_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f042557-07ce-4fe7-9543-0cda006f70d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining logic functions\n",
    "As mentioned earlier, LTN introduces the term <i>grounding</i>: defining abstract human concepts using functions and tensors. Since we are working with probabilities, there are many ways to define the logical operators. This happens because each logical operator acts like a function that takes two parameters, and there is no set way of computing that result. However, this is not a bad thing: we can play around, define them more loosely or more strictly.  \n",
    "We will use the recommended ones (from the official documentation), but if needed, we might change them later.  \n",
    "We will also define an aggregator which we will use in order to compute the satisfiability value of our constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8f6c36b-25e0-4033-a8de-38975ec2fb9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Equiv = ltn.Wrapper_Connective(ltn.fuzzy_ops.Equiv(ltn.fuzzy_ops.And_Prod(),ltn.fuzzy_ops.Implies_Reichenbach()))\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=4),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(p=6),semantics=\"exists\")\n",
    "\n",
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c213b-a3a9-4795-a8ab-49a9983b3623",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining axioms\n",
    "The beauty of Logic Tensor Networks is that we can define rules and constraints in a human readable way.  \n",
    "Some rules of clustering would be:\n",
    "* every element should be assigned to a cluster\n",
    "* every cluster should contain at least one element\n",
    "In addition to these 2 obvious rules, we need to define how to handle the relationships between the elements and clusters.  \n",
    "We have more ways to do so, and we will discuss each of them now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd780db-2549-476e-8a3c-4ad0594c7bf5",
   "metadata": {},
   "source": [
    "#### Approach #1:\n",
    "\n",
    "We use the euclidean distance to measure the similarity between two sets of answers. In order to transform it into a probability that can be used, we can define a Predicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2990e089-2991-465c-9351-5118f4db7506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Sim = ltn.Predicate.Lambda(\n",
    "    lambda args: tf.exp(-1.*tf.sqrt(tf.reduce_sum(tf.square(args[0]-args[1]),axis=1)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446837d-c47e-48e3-9a6f-488ddcc3a80b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we define the following rules (axioms):\n",
    "* x and y are two variables from the current batch\n",
    "* for each variable x, there exists a cluster that contains x\n",
    "* for each cluster, there exists a variable x that is a part of the cluster\n",
    "* for all variables x and y, the amount of similarity between x and y determines (implies) if the x and y are in the same cluster (x in cluster <=> y in cluster)\n",
    "In the end, we need to return how much these conditions hold.\n",
    "\n",
    "Translating these rules into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f9f4b0-b320-4878-89b6-a3957042e055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def axioms(p_exists, batch):\n",
    " \n",
    "    x = ltn.Variable(\"x\", batch)\n",
    "    y = ltn.Variable(\"y\", batch)    \n",
    " \n",
    "    axioms = [\n",
    "        Forall(x, Exists(cluster, C([x,cluster]),p=p_exists)),\n",
    "        Forall(cluster, Exists(x, C([x,cluster]),p=p_exists)),\n",
    "        Forall(\n",
    "            [x, y, cluster],\n",
    "            Implies( Sim([x, y]),\n",
    "                    Equiv(C([x, cluster]),C([y, cluster]))\n",
    "                    )\n",
    "        )\n",
    "    ]\n",
    "    kb = formula_aggregator(axioms)\n",
    "    sat = kb.tensor\n",
    "    return sat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ad729-4322-4430-86c2-4a58387fd800",
   "metadata": {},
   "source": [
    "#### Approach #2: \n",
    "\n",
    "We will still use the euclidean distance. However, this time we don't transform it into a predicate. We will use however two more threshold distances that will help the network decide if two elements need to be in the same cluster or different clusters.  \n",
    "We can also declare a predicate that turns the difference between two distances into a probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183bf65c-ae67-4954-a931-e2685d0dc1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eucl_dist = ltn.Function.Lambda(lambda inputs: tf.expand_dims(tf.norm(inputs[0]-inputs[1],axis=1),axis=1))\n",
    "is_greater_than = ltn.Predicate.Lambda(lambda inputs: inputs[0] > inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871749b-9296-4234-bff7-2cf5e619faf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "We want to define two limits for the distance between two elements:\n",
    "* the first limit indicates that any two elements that have the distance under the threshold need to be in the same clusters\n",
    "* the second limits indicates that any two elements that have the distance above the threshold need to be in different clusters  \n",
    "\n",
    "How do we choose the limits?  \n",
    "Actually, it's kind of experimental, because we don't know from the start those values.  \n",
    "A good starting point would be to think how much the answers differ for two people that have the same personality.\n",
    "We can say, since we only have 4 quite distinct personalities, that if the difference between answers is on average a maximum of 2, they probably belong in the same cluster.  \n",
    "We have a penalty of 2^2 per question in this case, and 60 questions per entry, so a penalty of 240. Since we use the euclidean distance, sqrt(240) = 15.491  \n",
    "Similarly, a difference of 4 would be way too much, so we can set the treshold to be sqrt(4^2 * 60) ≈ 31:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a4c952a-6cd7-4e24-8c98-11f01854e628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "close_threshold = 15.5\n",
    "distant_threshold = 31\n",
    "\n",
    "close_thr = ltn.Constant(close_threshold, trainable=False)\n",
    "distant_thr = ltn.Constant(distant_threshold, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41359363-c9d0-46d3-b3c2-39f31de109f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we define the following rules (axioms):\n",
    "* x and y are two variables from the current batch\n",
    "* for each variable x, there exists a cluster that contains x\n",
    "* for each cluster, there exists a variable x that is a part of the cluster\n",
    "* for each x, y and cluster, if the points are close to each other, they should belong to the same cluster\n",
    "* for each x, y and cluster, if the points are far from each other, they should belong to different clusters\n",
    "\n",
    "In the end, we need to return how much these conditions hold.\n",
    "\n",
    "Translating these rules into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6230266f-3863-4c08-90ab-6edcedfb6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def axioms(p_exists, batch):\n",
    "    x = ltn.Variable(\"x\", batch)\n",
    "    y = ltn.Variable(\"y\", batch)    \n",
    "\n",
    "    axioms = [\n",
    "        Forall(x, Exists(cluster, C([x,cluster]),p=p_exists)),\n",
    "        Forall(cluster, Exists(x, C([x,cluster]),p=p_exists)),\n",
    "        Forall([cluster,x,y], Equiv(C([x,cluster]),C([y,cluster])),\n",
    "            mask = is_greater_than([close_thr,eucl_dist([x,y])])),\n",
    "        Forall([cluster,x,y], Not(And(C([x,cluster]),C([y,cluster]))),\n",
    "            mask = is_greater_than([eucl_dist([x,y]),distant_thr]))\n",
    "    ]\n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    return sat_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9dc3c",
   "metadata": {},
   "source": [
    "### 3rd Method \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d50e3e48-fae7-4542-9c2a-bd46a4cacefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. -2. -3. -1.  2. -2.  0.  3.  0. -2.  0. -2.  1.  1. -2. -2.  1.\n",
      "   0.  3.  1.  2.  0.  0.  1. -2. -2.  0. -2.  1.  2.  0.  0.  0. -1. -1.\n",
      "   1.  2.  1. -1. -1.  2. -1.  1.  2.  0.  1.  0.  1.  0.  0.  0. -2.  0.\n",
      "   2.  0. -1. -1. -1.  3.]]\n",
      "{'ISTP': array([ 0., -1.,  3., -1.,  0.,  0., -2.,  0., -2.,  0.,  2., -1., -1.,\n",
      "        1., -1., -2., -1., -3.,  2.,  1.,  0., -1.,  0.,  0.,  0.,  2.,\n",
      "        0.,  0., -2.,  1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  3.,  2.,\n",
      "        0.,  2.,  0.,  0.,  0.,  0.,  0., -2.,  0., -1., -1.,  0.,  0.,\n",
      "        0., -1., -1.,  0.,  1.,  0., -2., -1.]), 'ENFJ': array([ 0.,  0., -1.,  0.,  2., -1., -2.,  0.,  1.,  0.,  0., -1., -1.,\n",
      "        0., -1., -2.,  2., -1.,  1.,  2.,  0.,  1., -1.,  0., -1.,  2.,\n",
      "       -1.,  0.,  0.,  2.,  3.,  0., -1.,  0.,  1.,  0., -2., -1.,  2.,\n",
      "       -2., -1., -1.,  3.,  0., -2.,  0., -1.,  0., -1.,  0.,  0.,  0.,\n",
      "        1.,  0.,  2.,  0.,  1., -1.,  2., -1.]), 'ENTP': array([ 0.,  0.,  0.,  0., -2., -1., -1.,  0., -2.,  0., -1.,  1.,  0.,\n",
      "        1., -1.,  2.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1., -2.,\n",
      "       -1.,  0., -2., -2., -2.,  0., -1.,  0., -1.,  2.,  2., -2., -2.,\n",
      "        0., -3., -3.,  0., -2.,  2.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,\n",
      "        1.,  0.,  1.,  1.,  0.,  1., -1., -3.]), 'ISFP': array([ 0.,  0., -2., -3., -1.,  2., -2.,  0.,  3.,  0., -2.,  0., -2.,\n",
      "        1.,  1., -2., -2.,  1.,  0.,  3.,  1.,  2.,  0.,  0.,  1., -2.,\n",
      "       -2.,  0., -2.,  1.,  2.,  0.,  0.,  0., -1., -1.,  1.,  2.,  1.,\n",
      "       -1., -1.,  2., -1.,  1.,  2.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,\n",
      "       -2.,  0.,  2.,  0., -1., -1., -1.,  3.])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Given keys\n",
    "given_keys = ['ISTP', 'ENFJ', 'ENTP', 'ISFP']\n",
    "\n",
    "# Create a dictionary with given keys and empty values\n",
    "my_labels = {key: None for key in given_keys}\n",
    "\n",
    "labels_found = 0 \n",
    "print(data_to_train_on[:1])\n",
    "for elem_idx in range(len(data_to_train_on)):\n",
    "    if labels_found == 4:\n",
    "        break\n",
    "\n",
    "    if my_labels[labels[elem_idx]] is None:\n",
    "        my_labels[labels[elem_idx]] = data_to_train_on[elem_idx]\n",
    "        labels_found += 1\n",
    "\n",
    "print(my_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71453709",
   "metadata": {},
   "outputs": [],
   "source": [
    "istp = ltn.Constant(my_labels['ISTP'], trainable = False)\n",
    "enfj = ltn.Constant(my_labels['ENFJ'], trainable = False)\n",
    "entp = ltn.Constant(my_labels['ENTP'], trainable = False)\n",
    "isfp =  ltn.Constant(my_labels['ISFP'], trainable = False)\n",
    "istp_label = ltn.Constant(0, trainable = False)\n",
    "enfj_label = ltn.Constant(1, trainable = False)\n",
    "entp_label = ltn.Constant(2, trainable = False)\n",
    "isfp_label = ltn.Constant(3, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdca1745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def axioms(p_exists, batch):\n",
    " \n",
    "    x = ltn.Variable(\"x\", batch)\n",
    "    y = ltn.Variable(\"y\", batch)    \n",
    " \n",
    "    axioms = [\n",
    "        C([istp, istp_label]),\n",
    "        C([enfj, enfj_label]),\n",
    "        C([entp, entp_label]),\n",
    "        C([isfp, isfp_label]),\n",
    "        Forall(x, Exists(cluster, C([x,cluster]),p=p_exists)),\n",
    "        Forall(\n",
    "            [x, y, cluster],\n",
    "            Implies( Sim([x, y]),\n",
    "                    Equiv(C([x, cluster]),C([y, cluster]))\n",
    "                    )\n",
    "        )\n",
    "    ]\n",
    "    kb = formula_aggregator(axioms)\n",
    "    sat = kb.tensor\n",
    "    return sat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c8a53-1a7a-4d9e-a486-947ae3e42404",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "First, we need to initialize the process, we'll just use the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3251e78f-a28a-47ef-9043-2ac3dc188322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.21772999>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch = batches[0]\n",
    "axioms(6, first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51eeea-3de5-44b3-86cd-d56181b936be",
   "metadata": {},
   "source": [
    "Next, we define a function that helps us output the average sat throughout the batches, mostly for outputing the progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7667edf8-403b-4c89-8e11-557ffe5eb3a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_avg_sat():\n",
    "    avg = 0.\n",
    "    for batch in batches:\n",
    "        avg += axioms(p_exists, batch)\n",
    "    avg = avg / len(batches)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e1fc8-344b-4e35-83f2-ae626ed3465a",
   "metadata": {},
   "source": [
    "Now we need to define the learning rate, and start the training process.  \n",
    "We played around a bit with the learning rate, and anything more than 0.01 is too high, and under 0.001 is a bit low. For our case, 0.001 looks like to be the sweet spot.  \n",
    "\n",
    "For each step, we need to set the p_exists (parameter for how loose the axioms are), iterate through all the batches, then ajust the weights based on the loss function: 1. - axioms(p_exists, batch)  \n",
    "After each 100 steps, we will display the current status, using the function from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d135dd29-57f6-4431-8937-d19f94ef1697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Sat Level 0.343\n",
      "Epoch 100: Sat Level 0.346\n",
      "Epoch 200: Sat Level 0.416\n",
      "Training finished at Epoch 299 with Sat Level 0.416\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainable_variables = C.trainable_variables\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    " \n",
    "\n",
    "for epoch in range(300):\n",
    "    if epoch <= 100:\n",
    "        p_exists = 1\n",
    "    else:\n",
    "        p_exists = 6\n",
    "    for batch in batches:\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = 1. - axioms(p_exists, batch)\n",
    "        grads = tape.gradient(loss_value, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    " \n",
    "    if epoch%100 == 0:\n",
    "        print(\"Epoch %d: Sat Level %.3f\"%(epoch, calculate_avg_sat()))\n",
    "print(\"Training finished at Epoch %d with Sat Level %.3f\"%(epoch, calculate_avg_sat()))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae1e63-04ff-492a-9291-42ecc1762f11",
   "metadata": {},
   "source": [
    "### Getting the results\n",
    "\n",
    "Usually, you would need to have separate training data and test data. However, since we provided no labels, we can use the same training data in order to check the clusterization results.\n",
    "We test each input individually, retreiving the output data from the network. The value of each output node indicates the likelyhood that the input is assigned to the coresponding cluster. Obviously, we will consider the node that has the highest probability as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1aec0bf5-fd7f-460e-b497-8affd5fc48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for input in data_to_train_on:\n",
    "    current_prediction_tensors = [C.model([tf.constant([input]), tf.constant([[i]])]) for i in clst_ids]\n",
    "    current_prediction = [tensor.numpy()[0][0] for tensor in current_prediction_tensors]\n",
    "    predictions.append(current_prediction)\n",
    " \n",
    "predicted_labels = []\n",
    "for prediction in predictions:\n",
    "    predicted_cluster = np.array(prediction).argmax() + 1\n",
    "    predicted_labels.append(predicted_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc04f6f-6967-4b82-ad51-d040ecf34c08",
   "metadata": {},
   "source": [
    "### Checking the accuracy\n",
    "\n",
    "We spent a bit of time deciding on how to define the accuracy, since the SAT value only indicates that clusters have been made, not if they are correctly coresponding to the actual labels.  \n",
    "We decided that for each pair of people that have the same label, we will check if they are in the same clusters. Counting the number of the positive and the number of negative responses, we can get a percentage that would be our success rate, or the Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb0936dc-4b30-4a6e-b414-3d9932bb685f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching clusters in proportion of  0.3250041514118754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "misses = 0.\n",
    "matches = 0.\n",
    "\n",
    "data_size = len(data_to_train_on)\n",
    "for index1 in range(data_size):\n",
    "    for index2 in range(index1 + 1, data_size):\n",
    "        if predicted_labels[index1] == predicted_labels[index2]:\n",
    "            if labels[index1] == labels[index2]:\n",
    "                matches += 1\n",
    "            else:\n",
    "                misses += 1\n",
    "\n",
    "print(\"Matching clusters in proportion of \", matches / (matches + misses))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ff4f7-70d2-4d06-a65a-431f167be087",
   "metadata": {},
   "source": [
    "### More information\n",
    "\n",
    "Accuracy does not tell everything. It also does not help us improve the network in any way. Therefore, we need a way to visualize the data, the clusters, and compare them to the original labels.  \n",
    "We'll use a method that transforms arrays in a way that they can be plotted in a 2-dimensional environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd9663d5-d1eb-402f-bbb6-83985245ba9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'ISFP' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m numeric_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[0;32m----> 9\u001b[0m     numeric_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(reduced_data[:, \u001b[38;5;241m0\u001b[39m], reduced_data[:, \u001b[38;5;241m1\u001b[39m], c\u001b[38;5;241m=\u001b[39mnumeric_labels, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClustering Results PCA - reale\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: 'ISFP' is not in list"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(data_to_train_on)\n",
    "\n",
    "l = ['ESFP', 'ENTJ', 'INFP', 'ISTJ']\n",
    "numeric_labels = []\n",
    "for elem in labels:\n",
    "    numeric_labels.append(l.index(elem) + 1)\n",
    "\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=numeric_labels, cmap='viridis')\n",
    "plt.title('Clustering Results PCA - Original clusters')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(data_to_train_on)\n",
    "\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=predicted_labels, cmap='viridis')\n",
    "plt.title('Clustering Results PCA - Predicted clusters')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebec32d-b948-4604-b0ae-d1a3c7441c94",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
